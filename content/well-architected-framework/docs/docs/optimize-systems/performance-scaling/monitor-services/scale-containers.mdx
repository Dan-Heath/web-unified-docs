---
page_title: Scale container workloads and orchestrators
description: Learn how to scale container workloads and orchestrators.
---

# Scale container workloads and orchestrators

If you are using a container orchestrator such as Kubernetes or Nomad, you can configure your workloads to scale automatically depending on the current load demand. Containerized workloads are often quicker to scale and offer more flexibility than scaling virtual machines, and lets you dynamically meet the resource requirements for your applications.

We recommend that you manage and monitor your container orchestrator the same way that you manage your other servers. This lets you build and scale your Nomad and Kubernetes clusters with Terraform just as you would manage the containers you run with them.

HashiCorp resources:

- [Kubernetes provider Horizontal Pod Autoscaler resource](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/horizontal_pod_autoscaler)
- [Dynamically Resize with Nomad Autoscaler](/nomad/tutorials/autoscaler)
- [Manage Kubernetes resources via Terraform](/terraform/tutorials/kubernetes/kubernetes-provider)
- [Deploy applications with the Helm provider](/terraform/tutorials/kubernetes/helm-provider)
- [Deploy infrastructure with the Terraform Cloud Operator v2](/terraform/tutorials/kubernetes/kubernetes-operator-v2) 
- [Manage Kubernetes with Terraform tutorial collection](/terraform/tutorials/kubernetes) 
- [Nomad cluster setup with Terraform](/nomad/tutorials/cluster-setup) 
- [Monitoring Nomad](/nomad/docs/operations/monitoring-nomad) 

## Next steps

In this section of [Monitor infrastructure](/well-architected-framework/optimize-systems/performance-scaling/monitor-infrastructure), you learned about scaling container workloads and orchestrators. Performance and scaling is part of the [Optimize systems pillar](/well-architected-framework/optimize-systems).